{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a2d533-01ed-4445-9725-99d1e8f71e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Homework 5: model deployment -- Yuxi Sun\n",
    "\n",
    "Instructions: Using the F1 dataset, build a predictive model and log it in MLflow and write the ML model predictions into a database.\n",
    "\n",
    "[20 pts] Create two (2) new tables in your own database where you'll store the predictions from each model for this exercise.\n",
    "[30 pts] Build two (2) predictive models using MLflow, logging hyperparameters, the model itself, four metrics, and two artifcats. Submit your MLflow experiments as part of your assignments\n",
    "[30 pts] For each model, store its predictions in the corresponding table you created in your own database. Ensure you are using your own database to store your predictions.\n",
    "[20 pts] Push your code to GitHub upon completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c8da87-2134-4eed-b069-8482d0faa41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database `f1_db_hw5` is ready.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 02:59:18 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/04/29 02:59:50 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/2987547044087220/1f5661645e95461fa266b826d2f60f41/artifacts/model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n2025/04/29 03:00:15 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/04/29 03:00:44 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/2987547044087220/cc032494fc4a4f6c9be6cf601696f141/artifacts/model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import mlflow, mlflow.spark\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix)\n",
    "\n",
    "# paths\n",
    "S3_PATH = \"s3a://columbia-gr5069-main/raw/\"\n",
    "MLFLOW_EXPERIMENT = \"/Users/ys3874@columbia.edu/HW5/f1-finishpos-podium\" \n",
    "DB_NAME = \"f1_db_hw5\"\n",
    "\n",
    "# generate database\n",
    "def create_databricks_db(spark):\n",
    "    spark.sql(f\"\"\"\n",
    "      CREATE DATABASE IF NOT EXISTS {DB_NAME}\n",
    "      COMMENT 'Holds F1 model prediction tables';\n",
    "    \"\"\")\n",
    "    print(f\"Database `{DB_NAME}` is ready.\")\n",
    "\n",
    "# data process\n",
    "def prepare_data(spark):\n",
    "    df_races   = spark.read.csv(f\"{S3_PATH}/races.csv\",   header=True, inferSchema=True)\n",
    "    df_results = spark.read.csv(f\"{S3_PATH}/results.csv\", header=True, inferSchema=True)\n",
    "    df = (\n",
    "        df_results.alias(\"r\")\n",
    "          .join(df_races.alias(\"c\"), F.col(\"r.raceId\")==F.col(\"c.raceId\"))\n",
    "          .select(\"r.raceId\",\"r.driverId\",\"r.grid\",\"r.laps\",\"r.milliseconds\",\n",
    "                  \"r.positionOrder\",\"r.points\")\n",
    "    )\n",
    "\n",
    "    # regression df: finishing position\n",
    "    df_reg = (\n",
    "        df.filter(F.col(\"positionOrder\").isNotNull())\n",
    "          .select(\n",
    "            \"raceId\",\"driverId\",\n",
    "            F.col(\"grid\").cast(FloatType()),\n",
    "            F.col(\"laps\").cast(FloatType()),\n",
    "            F.col(\"milliseconds\").cast(FloatType()),\n",
    "            F.col(\"positionOrder\").cast(FloatType()).alias(\"label\")\n",
    "          )\n",
    "          .na.fill(0, subset=[\"grid\",\"laps\",\"milliseconds\"])\n",
    "    )\n",
    "    train_reg, test_reg = df_reg.randomSplit([0.7,0.3], seed=42)\n",
    "\n",
    "    # classification df: podium\n",
    "    df_clf = (\n",
    "        df.filter(F.col(\"positionOrder\").isNotNull())\n",
    "          .select(\n",
    "            \"raceId\",\"driverId\",\n",
    "            F.col(\"grid\").cast(FloatType()),\n",
    "            F.col(\"laps\").cast(FloatType()),\n",
    "            F.col(\"milliseconds\").cast(FloatType()),\n",
    "            F.when(F.col(\"positionOrder\")<=3,1).otherwise(0).alias(\"label\")\n",
    "          )\n",
    "          .na.fill(0, subset=[\"grid\",\"laps\",\"milliseconds\"])\n",
    "    )\n",
    "    train_clf, test_clf = df_clf.randomSplit([0.7,0.3], seed=42)\n",
    "\n",
    "    return train_reg, test_reg, train_clf, test_clf\n",
    "\n",
    "# regression log and write (finish position)\n",
    "def log_and_write_regressor(spark, train_df, test_df):\n",
    "    assembler = VectorAssembler(inputCols=[\"grid\",\"laps\",\"milliseconds\"], outputCol=\"features\")\n",
    "\n",
    "    # decision tree regressor, used my own hyperparameters\n",
    "    dt_reg = DecisionTreeRegressor(\n",
    "        featuresCol=\"features\", labelCol=\"label\",\n",
    "        maxDepth=6, maxBins=64, minInfoGain=0.05\n",
    "    )\n",
    "    pipe = Pipeline(stages=[assembler, dt_reg])\n",
    "\n",
    "    with mlflow.start_run(run_name=\"DTReg_FinishingPosition\") as run:\n",
    "        run_id = run.info.run_id\n",
    "\n",
    "        # log parameters\n",
    "        mlflow.log_params({\"maxDepth\": 6, \"maxBins\": 64, \"minInfoGain\": 0.05})\n",
    "\n",
    "        model = pipe.fit(train_df)\n",
    "        preds = model.transform(test_df)\n",
    "        pdf = preds.select(\"label\",\"prediction\").toPandas()\n",
    "\n",
    "        # metrics\n",
    "        mse  = mean_squared_error(pdf[\"label\"], pdf[\"prediction\"])\n",
    "        mae  = mean_absolute_error(pdf[\"label\"], pdf[\"prediction\"])\n",
    "        r2   = r2_score(pdf[\"label\"], pdf[\"prediction\"])\n",
    "        rmse = mse ** 0.5\n",
    "\n",
    "        #log metrics\n",
    "        mlflow.log_metrics({\"mse\":mse, \"mae\":mae, \"rmse\":rmse, \"r2\":r2})\n",
    "\n",
    "        # artifact 1: feature importances CSV\n",
    "        feats = [\"grid\",\"laps\",\"milliseconds\"]\n",
    "        fi = model.stages[-1].featureImportances.toArray()\n",
    "        df_fi = pd.DataFrame({\"feature\":feats, \"importance\":fi})\n",
    "        tmp_csv = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "        df_fi.to_csv(tmp_csv.name, index=False)\n",
    "        mlflow.log_artifact(tmp_csv.name, artifact_path=\"feature_importances\") # log csv\n",
    "        os.unlink(tmp_csv.name)\n",
    "\n",
    "        # artifact 2: scatter plot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(pdf[\"label\"], pdf[\"prediction\"], alpha=0.5)\n",
    "        ax.set_xlabel(\"Actual Position\"); ax.set_ylabel(\"Predicted Position\")\n",
    "        ax.set_title(\"Actual vs. Predicted\")\n",
    "        tmp_png = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
    "        fig.savefig(tmp_png.name); plt.close(fig)\n",
    "        mlflow.log_artifact(tmp_png.name, artifact_path=\"plots\") # log scatter plot\n",
    "        os.unlink(tmp_png.name)\n",
    "\n",
    "        mlflow.spark.log_model(model, artifact_path=\"model\") # log model\n",
    "\n",
    "        # write into table\n",
    "        spark.sql(f\"USE {DB_NAME}\")\n",
    "        preds_to_write = preds.select(\n",
    "            \"raceId\",\"driverId\",\n",
    "            F.col(\"prediction\"),\n",
    "            F.col(\"label\").alias(\"actual\")\n",
    "        ).withColumn(\"run_id\", F.lit(run_id)).withColumn(\"prediction_time\", F.current_timestamp())\n",
    "\n",
    "        preds_to_write.write.mode(\"append\").saveAsTable(f\"{DB_NAME}.finishing_position_predictions\")\n",
    "\n",
    "# classifier log and write (podium)\n",
    "def log_and_write_classifier(spark, train_df, test_df):\n",
    "    assembler = VectorAssembler(inputCols=[\"grid\",\"laps\",\"milliseconds\"], outputCol=\"features\")\n",
    "\n",
    "    # decision tree classifier, used my own hyperparameters\n",
    "    dt_clf = DecisionTreeClassifier(\n",
    "        featuresCol=\"features\", labelCol=\"label\",\n",
    "        maxDepth=5, maxBins=32, minInfoGain=0.01\n",
    "    )\n",
    "    pipe = Pipeline(stages=[assembler, dt_clf])\n",
    "\n",
    "    with mlflow.start_run(run_name=\"DTClf_Podium\") as run:\n",
    "        run_id = run.info.run_id\n",
    "\n",
    "        # log parameters\n",
    "        mlflow.log_params({\"maxDepth\":5, \"maxBins\":32, \"minInfoGain\":0.01})\n",
    "\n",
    "        model = pipe.fit(train_df)\n",
    "        preds = model.transform(test_df)\n",
    "        pdf = preds.select(\"label\",\"prediction\").toPandas()\n",
    "\n",
    "        # metrics\n",
    "        acc  = accuracy_score(pdf[\"label\"], pdf[\"prediction\"])\n",
    "        prec = precision_score(pdf[\"label\"], pdf[\"prediction\"], zero_division=0)\n",
    "        rec  = recall_score(pdf[\"label\"], pdf[\"prediction\"], zero_division=0)\n",
    "        f1   = f1_score(pdf[\"label\"], pdf[\"prediction\"], zero_division=0)\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metrics({\"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1})\n",
    "\n",
    "        # artifact 1: confusion matrix\n",
    "        cm = confusion_matrix(pdf[\"label\"], pdf[\"prediction\"])\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.matshow(cm)\n",
    "        ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "        tmp_png = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
    "        fig.savefig(tmp_png.name); plt.close(fig)\n",
    "        mlflow.log_artifact(tmp_png.name, artifact_path=\"plots\") # log confusion matrix\n",
    "        os.unlink(tmp_png.name)\n",
    "\n",
    "        # artifact 2: predictions CSV\n",
    "        tmp_csv = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "        pdf.to_csv(tmp_csv.name, index=False)\n",
    "        mlflow.log_artifact(tmp_csv.name, artifact_path=\"predictions\") # log predictions csv\n",
    "        os.unlink(tmp_csv.name)\n",
    "\n",
    "        mlflow.spark.log_model(model, artifact_path=\"model\") # log model\n",
    "\n",
    "        # write into table\n",
    "        spark.sql(f\"USE {DB_NAME}\")\n",
    "        preds_to_write = preds.select(\n",
    "            \"raceId\",\"driverId\",\n",
    "            F.col(\"prediction\"),\n",
    "            F.col(\"label\").alias(\"actual\")\n",
    "        ).withColumn(\"run_id\", F.lit(run_id)).withColumn(\"prediction_time\", F.current_timestamp())\n",
    "\n",
    "        preds_to_write.write.mode(\"append\").saveAsTable(f\"{DB_NAME}.podium_predictions\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"F1ModelDeploymentDB\").getOrCreate()\n",
    "    mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    create_databricks_db(spark)\n",
    "    tr_reg, te_reg, tr_clf, te_clf = prepare_data(spark)\n",
    "\n",
    "    log_and_write_regressor(spark, tr_reg, te_reg)\n",
    "    log_and_write_classifier(spark, tr_clf, te_clf)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7508394704440455,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "hw5_yuxi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}